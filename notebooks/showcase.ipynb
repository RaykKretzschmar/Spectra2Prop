# Spectra2Prop Showcase

This notebook demonstrates the functionality of the **Spectra2Prop** project. We will:
1. Load the `SpectrumDataset` which generates synthetic MS/MS spectra.
2. Visualize a binned spectrum.
3. Initialize the 1D-CNN model.
4. Run a short training loop.
5. Perform inference to predict chemical superclasses.
import sys
import os

# Add project root to sys.path
sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))

print("Project root added to path.")
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

# Import our custom modules
from src.dataset import SpectrumDataset
from src.model import Spectra1DCNN

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
## 1. Data Loading and Visualization

We initialize the `SpectrumDataset`. In this demo, it generates synthetic spectral data (random peaks). In a real scenario, this would parse `.mgf` or `.mzML` files.

The raw peaks are "binned" into a fixed-length vector of size 2000.


# Initialize Dataset
# We use a small number of samples for this showcase
dataset = SpectrumDataset(num_samples=100, num_bins=2000, max_mz=1000)

# Get a single sample
vector_tensor, label = dataset[0]

# Convert to numpy for plotting
# Remove the channel dimension (1, 2000) -> (2000,)
spectrum_np = vector_tensor.squeeze().numpy()

# Plotting
plt.figure(figsize=(10, 4))
plt.plot(np.linspace(0, 1000, 2000), spectrum_np, color='black', linewidth=0.8)
plt.title(f"Binned MS/MS Spectrum (Label: {label.item()})")
plt.xlabel("m/z")
plt.ylabel("Intensity (Proc
## 2. Model Initialization

We instantiate the `Spectra1DCNN` model. The input dimension matches our bin count (2000), and we assume 31 ClassyFire superclasses.essed)")
model = Spectra1DCNN(input_dim=2000, num_classes=31).to(device)

print(model)

# Verify output shape with a dummy batch
dummy_input = vector_tensor.unsqueeze(0).to(device) # Add batch dim: (1, 1, 2000)
with torch.no_grad():
    output = model(dummy_input)
    ## 3. Mini-Training Loop

We will run a training loop for 5 epochs to demonstrate co
# Hyperparameters
BATCH_SIZE = 16
LR = 0.001
EPOCHS = 5

dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=LR)

model.train()
loss_history = []

print("Starting training...")
for epoch in range(EPOCHS):
    epoch_loss = 0
    for batch_idx, (spectra, labels) in enumerate(dataloader):
        spectra, labels = spectra.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(spectra)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
    
    avg_loss = epoch_loss / len(dataloader)
    loss_history.append(avg_loss)
    print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f}")

# Plot Loss
plt.plot(loss_history, marker='o')
plt.title("Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()nver
## 4. Inference and Probability Distribution

Finally, we take a spectrum, pass it through the trained model, and apply Softmax to get the probab
model.eval()

# Select a new sample
test_spectrum, test_label = dataset[5]
input_tensor = test_spectrum.unsqueeze(0).to(device) # (1, 1, 2000)

with torch.no_grad():
    logits = model(input_tensor)
    probabilities = torch.softmax(logits, dim=1)

# Get top 3 predictions
probs_np = probabilities.cpu().numpy().flatten()
top_indices = probs_np.argsort()[-3:][::-1] # Sort descending

print(f"True Label: {test_label.item()}")
print("-" * 30)
print("Top 3 Predictions:")
for idx in top_indices:
    print(f"Class {idx}: {probs_np[idx]*100:.2f}%")

# Visualize probability distribution
plt.figure(figsize=(10, 4))
plt.bar(range(31), probs_np)
plt.xlabel("ClassyFire Superclass ID")
plt.ylabel("Probability")
plt.title("Predicted Class Probability Distribution")
plt.show()ility distribution over the superclasses.gence.
print(f"\nOutput shape: {output.shape}  (Should be [1, 31])")
plt.grid(alpha=0.3)
plt.show()
